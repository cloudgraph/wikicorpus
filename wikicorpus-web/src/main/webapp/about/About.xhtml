<!DOCTYPE html [
    <!ENTITY nbsp "&#160;"> 
]>
<ui:composition template="/templates/main-layout.xhtml"
     xmlns="http://www.w3.org/1999/xhtml"
     xmlns:f="http://java.sun.com/jsf/core"
     xmlns:h="http://java.sun.com/jsf/html"
     xmlns:ui="http://java.sun.com/jsf/facelets"
     xmlns:p="http://primefaces.org/ui"
     xmlns:c="http://java.sun.com/jsp/jstl/core">
<ui:define name="content">
    <h:form id="about_frm">
    <style>
        td {
            white-space:normal !important;
        }
    </style>
    <p:spacer width="1" height="6"/>
    <p:panel style="width:100%;">
        <p>This example application transforms the roughly 5 million page, 1.5 billion word Wikipedia (English) text content into an NLP parsed corpus for Apache HBase™ using CloudGraph®, a suite of Java™ standards-based data-graph mapping and ad hoc query services for big-table sparse, columnar "cloud" databases.</p> 
        <p>No third party search engine or other specialized architecture is used, only Hadoop, MapReduce, HBase and the CloudGraph® HBase Service and MapReduce extensions.</p>
        <p>For more information on CloudGraph®, see <a href="http://cloudgraph.org">http://cloudgraph.org</a></p>
        <p>Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. 
         This application uses open source NLP software from the <a href="http://nlp.stanford.edu/software/index.shtml">Stanford NLP Group</a>. </p>
        <p>In general the Stanford NLP software transforms raw text into word dependency "trees" or graphs composed of sentences, word tokens and typed dependencies. The word tokens carry part-of-speech, character offsets, named entity and other tagging information depending on configuration settings, while the typed dependencies link the source or "governor" word to the target or "dependent" word through a rich set of hierarchically organized types such as modifiers (e.g. Adverbial Clause Modifier), arguments, auxiliaries and many others.</p>
        <p>Wiki pages very greatly in size, but the NLP graphs for each Wiki page are typically composed of tens-of-thousands of individual NLP nodes.</p>
        <p>While the NLP parsing results are amazingly rich and appear quite accurate, the parsing activity itself is extremely CPU and memory intensive, and parsing a vast amount of text such as Wikipedia in a reasonable time frame requires a large Hadoop cluster. We are however incrementally parsing and re-indexing nightly and are looking into other hardware options, but given the already large size of the corpus don't project any memory or performance problems as the corpus grows.</p>
        
        <p>Several MapReduce jobs are used which roughly fall into three areas.</p>
        <p>1.) Parse Wiki XML into plain text, generating word dependency trees using Stanford NLP (avg. 10K NLP nodes per wiki page</p>
        <p>2.) Reduce dependency trees to typed governor/dependent indexed aggregates with POS and other data.</p>
        <p>3.) Reduce word frequencies, other counts to indexed aggregates</p>

        <p>The MapReduce jobs leverage several CloudGraph® MapReduce extensions.</p>
        <p><b>MapReduce InputFormat Extensions</b></p>
        <p><a href="http://www.cloudgraph.org/apidocs/org/cloudgraph/hbase/mapreduce/GraphInputFormat.html">GraphInputFormat</a> – HBase scan(s), graph assembly and “recognition” from input Query. (i.e. detect properties deep within table rows)</p>
        <p><a href="http://www.cloudgraph.org/apidocs/org/cloudgraph/hbase/mapreduce/GraphXmlInputFormat.html">GraphXmlInputFormat</a> – Heterogeneous, arbitrary graphs unmarshalled from SDO XML</p>        
        <p><b>MapReduce OutputFormat Extensions</b></p>
        <p><a href="http://www.cloudgraph.org/apidocs/org/cloudgraph/hbase/mapreduce/GraphXmlOutputFormat.html">GraphXmlOutputFormat</a> – Heterogeneous data graphs marshalled to SDO XML</p>
        <p><b>MapReduce Mapper Extensions</b></p>
        <p><a href="http://www.cloudgraph.org/apidocs/org/cloudgraph/hbase/mapreduce/GraphMapper.html">GraphMapper</a> – consume fully assembled data graphs as (GraphWritable) – traverse and produce aggregates and/or output/persist new XML or data graphs</p>
        <p><b>MapReduce Reducer Extensions</b></p>
        <p><a href="http://www.cloudgraph.org/apidocs/org/cloudgraph/hbase/mapreduce/GraphReducer.html">GraphReducer</a> – consume aggregates output/persist new XML or data graphs</p>
        <p><b>Counters</b></p>
        <p><a href="http://www.cloudgraph.org/apidocs/org/cloudgraph/hbase/mapreduce/Counters.html">Counters</a> - job/task graph node counts, assembly times, ...more</p>
        
        <p>Note on Copyrights. Wikipedia contain hundreds of millions of words of copyrighted material. The only way that their use is legal (under US Fair Use Law) is because of the limited "Keyword in Context" rendering. It's kind of like the "snippet defense" used by Google. They retrieve and index billions of words of copyright material, but they only allow end users to access "snippets" of this data from their servers.</p>
        <p>CloudGraph® is a trademark of TerraMeta Software Inc.</p>
    </p:panel>
    </h:form>
</ui:define>
</ui:composition>
